experiment_name: "perplexity_defense_example"
experiment_type: "evaluation"

environment:
  model_name_or_path: "AlignmentResearch/robust_llm_pythia-imdb-14m-mz-ada-v3"
  model_family: "pythia"
  decoder_name: "EleutherAI/pythia-14m"
  decoder_family: "pythia"
  decoder_revision: "main"

dataset:
  dataset_type: "AlignmentResearch/IMDB"
  n_train: 10
  n_val: 10

evaluation:
  batch_size: 128
  evaluation_attack:
    attack_type: "random_token"
    append_to_modifiable_chunk: true
    random_token_attack_config:
      max_iterations: 4

defense:
  defense_type: "perplexity"
  num_preparation_examples: 10
  perplexity_defense_config:
    batch_size: 4
