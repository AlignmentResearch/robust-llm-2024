# @package _global_
defaults:
- /training: DEFAULT
- /training/adversarial: DEFAULT
- /dataset: imdb
- /model: EleutherAI/pythia-14m
- _self_

experiment_type: "training"

dataset:
  n_train: 2000
  n_val: 200

training:
  logging_steps: 100
  model_save_path_prefix_or_hf: "hf"
  num_train_epochs: 1
  learning_rate: 1e-5
  batch_size: 8
  optimizer: "adafactor"
  save_strategy: "no"

  adversarial:
    num_examples_to_generate_each_round: 200
    # We actually don't vary the number of rounds, but specify
    # it in the .py because we want it to be part of the model's name
    num_adversarial_training_rounds: -1
