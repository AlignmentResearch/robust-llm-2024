experiment_name: "perplexity_defense_example"
experiment_type: "evaluation"

environment:
  model_name_or_path: "AlignmentResearch/robust_llm_pythia-word-length-14m-niki-ada-v1"
  model_family: "pythia"
  dataset_type: "word_length"
  train_set_size: 1000
  validation_set_size: 1000
  shuffle_validation_set: true
  decoder_name: "EleutherAI/pythia-14m"
  decoder_family: "pythia"
  decoder_revision: "main"

evaluation:
  batch_size: 128
  num_generated_examples: 1000
  evaluation_attack:
    attack_type: "random_token"
    random_token_attack_config:
      max_iterations: 40

defense:
  defense_type: "perplexity"
  num_preparation_examples: 50
  perplexity_defense_config:
    batch_size: 4
