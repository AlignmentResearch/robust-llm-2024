# @package _global_
defaults:
- /training: DEFAULT
- _self_

training:
  optimizer: "adafactor"
  # With an effective batch size of 8, logging_steps=240 should run eval
  # (20000/8)/240 ~= 10 times (since eval_steps is set to logging_steps by
  # default).
  logging_steps: 240
  model_save_path_prefix_or_hf: "hf"
  num_train_epochs: 3
  learning_rate: 1e-5
  lr_scheduler_type: "linear"
  save_strategy: "no"

dataset:
  n_train: 20_000
  n_val: 200

experiment_type: "training"
