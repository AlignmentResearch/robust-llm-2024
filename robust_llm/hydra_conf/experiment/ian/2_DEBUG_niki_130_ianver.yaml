# @package _global_
defaults:
- /training: DEFAULT
- /training/adversarial: DEFAULT
- /model: Default/clf/imdb/pythia-14m-s0
- /dataset: IMDB/stable
- /attack@training.adversarial.training_attack: RANDOM_TOKEN
- /attack@evaluation.evaluation_attack: RANDOM_TOKEN
- _self_

evaluation:
  num_iterations: 64

environment:
  logging_level: 10


training:
  optimizer: "adafactor"
  logging_steps: 240
  num_train_epochs: 1
  learning_rate: 1e-5
  lr_scheduler_type: "constant"
  save_to: "NONE"
  save_name: null
  save_prefix: "debug_outputs"
  save_strategy: "epoch"
  # save_steps: 10000
  save_total_limit: 5
  adversarial:
    num_examples_to_generate_each_round: 10
    max_augmented_data_size: 10
    num_adversarial_training_rounds: 4
    max_adv_data_proportion: 0.8
    loss_rank_weight: 0.5
    # DEBUG: Trying no loss_rank_weight because we aren't checkpointing it
    # loss_rank_weight: 0.0
    adv_sampling_decay: 0.005
    skip_first_training_round: true
    attack_schedule:
      start: 16
      end: 32

dataset:
  n_train: 100
  n_val: 20

experiment_type: "training"
