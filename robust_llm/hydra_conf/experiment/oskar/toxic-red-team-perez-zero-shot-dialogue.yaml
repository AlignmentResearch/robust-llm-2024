# @package _global_
defaults:
  - /dataset: pure_generation
  - /evaluation: DEFAULT
  - /model: meta-llama/Llama-2-7b-chat-hf
  - /attack@evaluation.evaluation_attack: lm-attack-toxicity-perez-zero-shot-dialogue
  - /callback@evaluation.final_success_binary_callback: toxicity_in_generation
  - _self_

dataset:
  n_val: 100

model:
  generation_config:
    max_new_tokens: 50
    do_sample: true
    top_p: 0.8
    top_k: 0 # Setting top_k=0 to disable top_k sampling effect


experiment_name: ???
run_name: ???
experiment_type: "evaluation"
