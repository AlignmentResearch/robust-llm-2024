from functools import cached_property
from typing import Any

from typing_extensions import override

from robust_llm.attacks.search_free.search_free import SearchFreeAttack
from robust_llm.config.attack_configs import LMAttackConfig
from robust_llm.models import WrappedModel
from robust_llm.rllm_datasets.modifiable_chunk_spec import (
    ChunkType,
    ModifiableChunkSpec,
)
from robust_llm.utils import get_randint_with_exclusions


class ZeroShotLMAttack(SearchFreeAttack):
    """Zero-shot language model red-teaming attack.

    This attack uses a pre-trained language model (the adversary) to generate attack
    tokens for a victim model. Unlike other attacks, we hope the attack tokens
    generated by the adversary will be more coherent and lower perplexity for the
    victim.
    """

    REQUIRES_TRAINING = False

    def __init__(
        self,
        attack_config: LMAttackConfig,
        victim: WrappedModel,
        run_name: str,
        logging_name: str | None = None,
    ) -> None:
        super().__init__(
            attack_config, victim=victim, run_name=run_name, logging_name=logging_name
        )

        self.adversary = WrappedModel.from_config(
            attack_config.adversary, accelerator=victim.accelerator
        )
        self.adversary.eval()
        if attack_config.n_its > 1:
            # If we're doing multiple iterations, we need to sample.
            # Otherwise we will always generate the same text.
            assert self.adversary.generation_config is not None
            assert self.adversary.generation_config.do_sample

        self.adversary_input_templates = attack_config.adversary_input_templates
        self.adversary_output_templates = attack_config.adversary_output_templates
        self.adversary_prefix = attack_config.adversary_prefix
        self.attack_start_strings = attack_config.attack_start_strings
        self.attack_end_strings = attack_config.attack_end_strings
        self.use_raw_adversary_input = attack_config.use_raw_adversary_input
        if self.is_classification_task:
            assert len(self.adversary_input_templates) == self.num_labels
        else:
            assert len(self.adversary_input_templates) == 1

    @property
    def num_labels(self) -> int:
        return getattr(self.victim.model, "num_labels", 0)

    @property
    def is_generative_task(self) -> bool:
        return self.num_labels == 0

    @property
    def is_classification_task(self) -> bool:
        return not self.is_generative_task

    @cached_property
    def shared_attack_tokens(self) -> list[list[int]]:
        raise NotImplementedError("multiprompt is not implemented for LM attacks")

    @override
    def post_process_attack_string(self, attack_tokens: str, chunk_index: int) -> str:
        """Post-process the attack string using the configured template.

        Args:
            attack_tokens: The attack tokens to post-process.
            chunk_index: The index of the chunk in the example.
        """
        return self.adversary_output_templates[chunk_index].format(attack_tokens)

    def get_target_label(self, chunk_label: int) -> int:
        """Returns a target classification output for the adversary model."""
        return (
            get_randint_with_exclusions(
                high=self.num_labels, exclusions=[chunk_label], rng=self.rng
            )
            if self.is_classification_task
            else 0
        )

    def apply_adversary_chat_template(self, text: str) -> str:
        conv = self.adversary.init_conversation()
        conv.append_user_message(text)
        conv.append_assistant_message(self.adversary_prefix)
        adversary_input = conv.get_prompt(skip_last_suffix=True)
        return adversary_input

    def delimit_attack(self, text: str) -> str:
        for start_string in self.attack_start_strings:
            if start_string in text:
                text = text[text.index(start_string) + len(start_string) :]
        for end_string in self.attack_end_strings:
            if end_string in text:
                text = text[: text.index(end_string)]
        return text

    def generate_from_text(
        self,
        adversary_input: str,
        current_iteration: int,
        chunk_seed: int,
    ) -> tuple[list[int], dict[str, Any]]:
        """Generates attack tokens using the adversary model."""
        # Ensure that generations are deterministic for a given seed/iteration combo
        self.adversary.seed = hash((chunk_seed, current_iteration))
        text = self.adversary.generate_from_text(adversary_input)
        attack = self.delimit_attack(text)
        token_ids = self.victim.tokenize(attack)["input_ids"]
        assert isinstance(token_ids, list)
        info = dict(
            adversary_input=adversary_input,
            current_iteration=current_iteration,
            chunk_seed=chunk_seed,
            adversary_output=text,
            attack_output=attack,
        )
        return token_ids, info

    @override
    def _get_attack_tokens(
        self,
        chunk_text: str,
        chunk_type: ChunkType,
        current_iteration: int,
        chunk_label: int,
        chunk_seed: int,
    ) -> tuple[list[int], dict[str, Any]]:
        """Returns the LM red-team attack tokens for the current iteration.

        For classification, we pick a random alternate target label to decide
        the template to use for the attack. For generation, we always use the
        same template.

        We pass the template text to the adversary model, along with the chunk,
        to generate some attack tokens.

        Args:
            chunk_text: The text of the chunk to be attacked.
            chunk_type: The type of the chunk to be attacked (not used).
            current_iteration: Used to determine the seed for adversary generation.
            chunk_label: The label of the chunk to be attacked (for classification).
            chunk_seed: The seed for the chunk to be attacked (for generation).

        Returns:
            The attack tokens for the current iteration.
        """
        assert isinstance(chunk_text, str)
        assert isinstance(chunk_type, ChunkType)
        target_label = self.get_target_label(chunk_label)
        if self.use_raw_adversary_input:
            adversary_input = self.adversary_input_templates[target_label]
        else:
            formatted_chunk = self.adversary_input_templates[target_label].format(
                chunk_text
            )
            adversary_input = self.apply_adversary_chat_template(formatted_chunk)
        return self.generate_from_text(adversary_input, current_iteration, chunk_seed)

    @override
    def _get_attacked_input(
        self,
        example: dict[str, Any],
        modifiable_chunk_spec: ModifiableChunkSpec,
        current_iteration: int,
    ) -> tuple[str, dict[str, Any]]:
        assert modifiable_chunk_spec.n_modifiable_chunks == len(
            self.adversary_output_templates
        )
        return super()._get_attacked_input(
            example, modifiable_chunk_spec, current_iteration
        )
