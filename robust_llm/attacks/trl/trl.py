import os
from collections.abc import Sequence
from typing import Any, Optional, cast

import torch
import wandb
from datasets import Dataset
from tqdm import tqdm
from trl import PPOTrainer
from typing_extensions import override

from robust_llm import logger
from robust_llm.attacks.attack import Attack
from robust_llm.attacks.trl.utils import (
    check_for_not_finite,
    make_ppo_trainer,
    prepare_adversary,
    prepare_prompts,
)
from robust_llm.config.attack_configs import TRLAttackConfig
from robust_llm.logging_utils import WandbTable
from robust_llm.models import WrappedModel
from robust_llm.rllm_datasets.modifiable_chunk_spec import ModifiableChunkSpec
from robust_llm.rllm_datasets.rllm_dataset import RLLMDataset
from robust_llm.scoring_callbacks import CallbackInput, build_tensor_scoring_callback

TRL_RESPONSE_STR = "<USER INPUT HERE>"


class TRLAttack(Attack):
    """Transformer Reinforcement Learning attack.

    Replaces all the modifiable text with random tokens
    from the tokenizer's vocabulary.
    """

    REQUIRES_TRAINING = True

    def __init__(
        self,
        attack_config: TRLAttackConfig,
        logging_name: str,
        victim: WrappedModel,
    ) -> None:
        """Constructor for TRLAttack.

        Args:
            attack_config: config of the attack
            logging_name: name of the attack; used for logging
            victim: the WrappedModel to be attacked
        """

        super().__init__(
            attack_config=attack_config,
            logging_name=logging_name,
        )

        # Check the logging frequency
        if self.attack_config.log_frequency is None:
            logger.warning(
                "If you want to log trl training stats, "
                "you need to set a positive log_frequency. "
                "As is, no trl train stats will be logged."
            )

        self.victim = victim
        cb_config = attack_config.rewards_from_victim_callback
        self.rewards_from_victim_callback = build_tensor_scoring_callback(cb_config)

        self.model_name_to_save = attack_config.model_name_to_save
        self.model_save_path_prefix = attack_config.model_save_path_prefix

        assert victim.accelerator is not None
        self.adversary = prepare_adversary(
            attack_config, victim.model.config.num_labels, victim.accelerator
        )

        # NOTE: these values are taken from the TRL quickstart example
        # and might not be optimal for this setting
        # defaults are here
        #   https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationConfig
        # TRL quickstart example is here
        #   https://huggingface.co/docs/trl/v0.7.4/en/quickstart#minimal-example
        self.generation_kwargs = {
            "min_length": attack_config.min_length,
            "top_k": 0.0,
            "top_p": 1.0,
            "do_sample": True,
            "pad_token_id": self.adversary.right_tokenizer.eos_token_id,
            "max_new_tokens": attack_config.max_new_tokens,
        }
        self.ppo_trainer: Optional[PPOTrainer] = None
        self.model_size = victim.n_params
        self.model_family = victim.family

    @override
    def train(
        self,
        dataset: RLLMDataset,
    ) -> None:
        assert isinstance(self.attack_config, TRLAttackConfig)
        self.ppo_trainer = make_ppo_trainer(
            attack_config=self.attack_config,
            adversary_model=self.adversary.model,
            # We use the right-padding tokenizer because it doesn't matter and
            # we already loaded the right-padding tokenizer from the victim.
            # TODO(ian): Check that it's actually fine to pass the right-padding
            # tokenizer.
            adversary_tokenizer=self.adversary.right_tokenizer,
            dataset=dataset.ds,
        )

        assert self.ppo_trainer is not None
        ppo_epochs: int = self.ppo_trainer.config.ppo_epochs
        table = WandbTable(f"{self.logging_name}/attack_table")
        for epoch in tqdm(range(ppo_epochs), "epoch: "):
            epoch_rewards: list[torch.Tensor] = []
            for batch in tqdm(self.ppo_trainer.dataloader):
                # Get the attacks from the adversary
                (
                    attacked_texts,
                    context_tensor_list,
                    responses,
                ) = self._get_attacked_texts(
                    batch,
                    modifiable_chunk_spec=dataset.modifiable_chunk_spec,
                )
                # Adjust the columns to be consistent with the new text
                # generated by the adversary.
                # TODO(ian): Clean this up if possible; it's a bit messy to
                # compute this all inside TRL, maybe it should be in
                # RLLMDataset.
                batch_ds = Dataset.from_dict(
                    {
                        "text": attacked_texts,
                        "clf_label": batch["clf_label"],
                        "gen_target": batch["gen_target"],
                    }
                )
                batch_ds = dataset.update_dataset_based_on_text(batch_ds)

                callback_input = CallbackInput(
                    input_data=attacked_texts,
                    clf_label_data=batch_ds["clf_label"],
                    gen_target_data=batch_ds["gen_target"],
                )
                rewards_out = self.rewards_from_victim_callback(
                    self.victim,
                    callback_input,
                )
                # TensorCallbackOut has its return tensors in the "losses" attribute.
                raw_rewards = rewards_out.losses

                # PPOTrainer requires a list of tensors.
                rewards = [torch.tensor(reward) for reward in raw_rewards]

                assert len(rewards) == len(context_tensor_list) == len(responses)
                # Specific tensor types are deprecated but PPO expects it.
                cast_queries = cast(list[torch.LongTensor], context_tensor_list)
                cast_responses = cast(list[torch.LongTensor], responses)
                cast_scores = cast(list[torch.FloatTensor], rewards)

                train_stats = self.ppo_trainer.step(
                    queries=cast_queries,
                    responses=cast_responses,
                    scores=cast_scores,
                )
                epoch_rewards.extend(rewards)

                # Check for not-finite values in the training stats
                check_for_not_finite(train_stats)

                # Log the ppo stats and update the logging counters
                self._maybe_log_trl(train_stats, rewards, table)

            average_reward = torch.Tensor(epoch_rewards).squeeze().mean()

            logger.info(
                "Training TRL; epoch %s had average reward %s", epoch, average_reward
            )

        self._maybe_save_model_to_path_or_hf()
        table.save()

    def _maybe_log_trl(
        self,
        train_stats: dict[str, Any],
        rewards: Sequence[torch.Tensor],
        table: WandbTable,
    ):
        self.logging_counter.increment(
            step_count_to_add=1,
            datapoint_count_to_add=len(rewards),
            commit=False,
        )

        if self.attack_config.log_frequency is not None:
            if self.logging_counter.step_count % self.attack_config.log_frequency == 0:
                prepended_train_stats = {
                    f"{self.logging_name}/{key}": value
                    for key, value in train_stats.items()
                }

                wandb.log(prepended_train_stats, commit=True)

                prepended_train_stats["attack_step_count"] = (
                    self.logging_counter.step_count
                )
                prepended_train_stats["attack_datapoint_count"] = (
                    self.logging_counter.datapoint_count
                )
                prepended_train_stats["global_step_count"] = (
                    self.logging_counter.root.step_count
                )
                prepended_train_stats["global_datapoint_count"] = (
                    self.logging_counter.root.datapoint_count
                )
                prepended_train_stats["model_size"] = self.model_size
                prepended_train_stats["model_family"] = self.model_family
                table.add_data(prepended_train_stats)

    @override
    def get_attacked_dataset(
        self,
        dataset: RLLMDataset,
    ) -> tuple[RLLMDataset, dict[str, Any]]:

        # At present, the trl attack is set up to only work
        # with one modifiable chunk
        assert dataset.modifiable_chunk_spec.n_modifiable_chunks == 1

        attacked_texts, _, _ = self._get_attacked_texts(
            dataset=dataset.ds,
            modifiable_chunk_spec=dataset.modifiable_chunk_spec,
        )
        # TODO(ian): Work out if 'apply_chat_template' messes with the updating
        # done in 'with_attacked_text'.
        attacked_texts = self.victim.maybe_apply_chat_template(attacked_texts)
        attacked_dataset = dataset.with_attacked_text(attacked_texts)
        return attacked_dataset, {}

    def _get_attacked_texts(
        self,
        dataset: Dataset,
        modifiable_chunk_spec: ModifiableChunkSpec,
    ) -> tuple[list[str], list[torch.Tensor], list[torch.Tensor]]:
        """The trl attack method itself.

        Args:
            dataset: The dataset to attack. Must have a "chunked_text" column.
            modifiable_chunk_spec: Specification which chunks of the original text can
            be modified, and how.

        Returns:
            attacked_texts: A list of the full attacked texts, ready to be
                passed to the victim model for classification. Made by combining
                the context with the adversary-generated responses.
            context_tensor_list: A list of tensors, each representing
                the context that was passed to the adversary model.
            adversary_generated_responses: A list of tensors, each representing
                the response generated by the adversary model
        """

        if self.ppo_trainer is None:
            raise ValueError(
                "You must train the TRLAttack before using it to generate attacks"
            )

        chunked_text = dataset["chunked_text"]
        contexts = prepare_prompts(
            text_chunked=chunked_text,
            response_text=TRL_RESPONSE_STR,
            modifiable_chunk_spec=modifiable_chunk_spec,
        )
        context_tensors = self.adversary.tokenize(
            contexts,
            # We use left-padding for autoregressive outputs.
            padding_side="left",
            return_tensors="pt",
        )
        input_ids = context_tensors["input_ids"]
        assert isinstance(input_ids, torch.Tensor)
        context_tensor_list = [context_tensor for context_tensor in input_ids]

        with torch.no_grad():
            adversary_generated_responses = self.ppo_trainer.generate(
                context_tensor_list,
                return_prompt=False,
                **self.generation_kwargs,
            )

        # Check that the ppo trainer's output is a list of lists of tensors
        assert isinstance(adversary_generated_responses, list)
        assert all(
            isinstance(response, torch.Tensor)
            for response in adversary_generated_responses
        )

        adversary_generated_responses_txt = self.adversary.batch_decode(
            adversary_generated_responses
        )

        attacked_texts = prepare_prompts(
            text_chunked=chunked_text,
            response_text=adversary_generated_responses_txt,
            modifiable_chunk_spec=modifiable_chunk_spec,
        )

        return (
            attacked_texts,
            context_tensor_list,
            adversary_generated_responses,
        )

    def _maybe_save_model_to_path_or_hf(self) -> None:
        # TODO(GH#508): Make sure saving works with accelerate
        if self.model_save_path_prefix is None:
            logger.warning("No model_save_path_prefix provided; not saving the model")
            return

        assert self.adversary.accelerator is not None
        if self.adversary.accelerator.is_main_process:
            assert wandb.run is not None
            output_dir = os.path.join(
                self.model_save_path_prefix, "models", self.model_name_to_save
            )
            wandb.run.summary["saved_dir"] = output_dir  # type: ignore
            logger.info("Saving the trl model to %s", output_dir)

            # TODO(niki): enable saving on hf hub
            self.adversary.model.save_pretrained(output_dir)
            self.adversary.right_tokenizer.save_pretrained(output_dir)
