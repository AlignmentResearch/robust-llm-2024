version: 2.1

orbs:
  python: circleci/python@1.5.0
  codecov: codecov/codecov@3.3.0

executors:
  python-executor:
    docker:
      - image: cimg/python:3.10.13

commands:
  prepare-virtualenv:
    description: "Create Python virtual environment with caching support"
    steps:
      - restore_cache:
          # To force cache validation, increase the version number N in "venv-vN" below
          key: &venv-cache venv-v3-{{ checksum "pyproject.toml" }}
      - run:
          name: Setup Virtual Environment
          command: |
            # Store venv activate script in special CircleCI BASH_ENV
            # variable which will be automatically sourced before
            # new commands
            echo ". $(pwd)/venv/bin/activate" >> "$BASH_ENV"
            if [ ! -d "venv" ]; then
              # venv does not exist: create it
              python -m venv venv
              # Activate venv now it's been created
              source "$BASH_ENV"
              # Install CPU version of PyTorch as CI does not have GPU.
              # (CUDA version runs on CPU OK, but file size is larger,
              # slowing down CI.)
              pip install torch --index-url https://download.pytorch.org/whl/cpu
              # Install remaining dependencies.
              # Note if we ever remove `-e` it'll become necessary to reinstall
              # robust_llm even if a virtual environment already exists, with:
              # pip install --upgrade --force-reinstall --no-deps '.[dev,tensorflow]'
              pip install -e '.[dev,tensorflow]'
            fi

      - save_cache:
          paths:
            - "venv"
          key: *venv-cache

      - run:
          name: print installed packages
          command: pip freeze --all

  memory-monitor:
    description: "Monitor memory usage of processes in background"
    steps:
      - run:
          name: Memory Monitor
          command: |
            mkdir /tmp/resource-usage
            export FILE=/tmp/resource-usage/memory.txt
            while true; do
              ps eo pid,%cpu,%mem,args,uname --sort=-%mem >> $FILE
              echo "----------" >> $FILE
              sleep 1
            done
          background: true

  store-test-output:
    description: "Store the output of tests."
    steps:
      - store_artifacts:
          path: /tmp/test-reports
          destination: test-reports

      - store_test_results:
          path: /tmp/test-reports

  store-memory-monitor:
    description: "Store resource usage of tests"
    steps:
      - store_artifacts:
          path: /tmp/resource-usage
          destination: resource-usage

jobs:
  pytest:
    executor: python-executor
    parallelism: 4
    resource_class: xlarge
    steps:
      - checkout
      - prepare-virtualenv
      - run:
          name: Show num vCPUs & processes
          command: |
            # See https://discuss.circleci.com/t/environment-variable-set-to-the-number-of-available-cpus/32670/3
            # for explanation of the weird expression below. -- Aaron
            NUM_CPUS=$(($(cat /sys/fs/cgroup/cpu/cpu.shares) / 1024))
            NUM_PROCS=$((NUM_CPUS / 2))
            echo ${NUM_CPUS} vCPUs, ${NUM_PROCS} processes
            # Save environment variable for next step
            echo "export NUM_PROCS=${NUM_PROCS}" >> $BASH_ENV
      - memory-monitor
      - run:
          name: Run tests
          command: |
            # Ensure that the script fails if any command fails
            set -euo pipefail

            # List all test functions in the specified directory
            TESTFUNCTIONS=$(pytest --collect-only -q | grep '::' | grep -v '<Module')
            echo "$TESTFUNCTIONS" > /tmp/node-ids.txt

            # Check for duplicates in TESTFUNCTIONS
            TOTAL_TESTFUNCTIONS=$(echo "$TESTFUNCTIONS" | wc -l)
            UNIQUE_TESTFUNCTIONS=$(echo "$TESTFUNCTIONS" | sort | uniq | wc -l)
            if [ "$TOTAL_TESTFUNCTIONS" -ne "$UNIQUE_TESTFUNCTIONS" ]; then
                echo "Error: TESTFUNCTIONS contains duplicates!"
                echo "Total test functions: $TOTAL_TESTFUNCTIONS"
                echo "Unique test functions: $UNIQUE_TESTFUNCTIONS"
                exit 1
            fi

            # Extract function names and save to a temporary file
            FUNCTIONNAMES=$(echo "$TESTFUNCTIONS" | sed -E 's/^.+:://')
            echo "$FUNCTIONNAMES" > /tmp/function-names.txt

            # Check for duplicates in FUNCTIONNAMES
            TOTAL_FUNCTIONNAMES=$(echo "$FUNCTIONNAMES" | wc -l)
            UNIQUE_FUNCTIONNAMES=$(echo "$FUNCTIONNAMES" | sort | uniq | wc -l)
            if [ "$TOTAL_FUNCTIONNAMES" -ne "$UNIQUE_FUNCTIONNAMES" ]; then
                echo "Error: FUNCTIONNAMES contains duplicates!"
                echo "Total test functions: $TOTAL_FUNCTIONNAMES"
                echo "Unique test functions: $UNIQUE_FUNCTIONNAMES"
                exit 1
            fi

            # Check if the number of test functions and function names matches
            if [ "$TOTAL_TESTFUNCTIONS" -ne "$TOTAL_FUNCTIONNAMES" ]; then
                echo "Error: Number of test functions and function names does not match!"
                echo "Total test functions: $TOTAL_TESTFUNCTIONS"
                echo "Total function names: $TOTAL_FUNCTIONNAMES"
                echo "List of test functions: $TESTFUNCTIONS"
                echo "List of function names: $FUNCTIONNAMES"
                exit 1
            fi

            # Split the function names by timings using CircleCI's test splitting feature
            SPLITFUNCTIONNAMES=$(circleci tests split --split-by=timings --timings-type=testname /tmp/function-names.txt)

            # Reconstruct the full test paths by matching the function names with the original list
            FULLTESTPATHS=$(echo "$SPLITFUNCTIONNAMES" | while read -r FUNCTIONNAME; do
              ESCAPED_FUNCTIONNAME=$(echo "$FUNCTIONNAME" | sed 's/[]\/$*.^|[]/\\&/g')
              grep "::$ESCAPED_FUNCTIONNAME$" /tmp/node-ids.txt;
            done)

            # Check if the number of split function names and full test paths matches
            TOTAL_SPLITFUNCTIONNAMES=$(echo "$SPLITFUNCTIONNAMES" | wc -l)
            TOTAL_FULLTESTPATHS=$(echo "$FULLTESTPATHS" | wc -l)

            if [ "$TOTAL_SPLITFUNCTIONNAMES" -ne "$TOTAL_FULLTESTPATHS" ]; then
                echo "Error: Number of split function names and full test paths does not match!"
                echo "Total split function names: $TOTAL_SPLITFUNCTIONNAMES"
                echo "Total full test paths: $TOTAL_FULLTESTPATHS"
                echo "List of split function names: $SPLITFUNCTIONNAMES"
                echo "List of full test paths: $FULLTESTPATHS"
                exit 1
            fi

            # Ensure the output directory exists
            mkdir -p /tmp/test-reports

            # Run pytest with the split test functions
            pytest --cov=robust_llm --cov=tests --junitxml=/tmp/test-reports/junit.xml --durations=500 -vv $FULLTESTPATHS
          environment:
            WANDB_MODE: offline
      - codecov/upload
      - store-test-output
      - store-memory-monitor

  lint:
    executor: python-executor
    steps:
      - checkout
      - prepare-virtualenv
      - run:
          name: black
          command: |
            black --version
            black . --check
      - run:
          name: isort
          command: |
            isort --version
            isort robust_llm --check --verbose
      - run:
          name: pyright
          command: |
            pyright --version
            pyright
      - run:
          name: mypy
          command: |
            mypy --version
            mypy --follow-imports=silent --show-error-codes --check-untyped-defs robust_llm/ tests/
      - run:
          name: autoflake
          command: |
            autoflake --version
            autoflake --in-place --remove-all-unused-imports --remove-duplicate-keys --remove-unused-variables -v --exclude venv --recursive .
      - run:
          name: flake8
          command: |
            flake8 --version
            flake8 . -v --exclude venv
      - run:
          name: pydocstyle
          command: |
            pydocstyle --version
            pydocstyle robust_llm --verbose

workflows:
  version: 2
  tests:
    jobs:
      - pytest
      - lint
