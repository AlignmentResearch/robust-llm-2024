# robust-llm

## Simple installation
If you just want to run the code and nothing else, you can do the following:

1. clone the repository
2. cd into it
3. create a new Python 3.10 virtual environment called `venv`
4. activate the virtual environment
5. install the `robust-llm` project
```
git clone https://github.com/AlignmentResearch/robust-llm.git
cd robust-llm
python -m venv venv
source venv/bin/activate
pip install .
```

Note that this project has not been tested with different versions of Python.

## Development installation

If you want to install `robust-llm` with developer dependencies (which gives you development tools like linting and tests), do the following:

1. Follow steps 1-4 from the [Simple installation](#simple-installation).

2. Add [pre-commit](https://pre-commit.com/) hooks for various linting tasks by installing pre-commit:
```
pre-commit install
```
3. Install `robust-llm` in developer mode, with dev dependencies:
```
pip install -e '.[dev]'
```


## Running experiments
Experiments are configured with [Hydra](https://hydra.cc/). You can run the default configuration with:

```
python robust_llm
```

Defaults can be overridden via the command line:

```
python robust_llm experiment.environment.seed=42
```

Alternatively, you can also define new config files, which is the recommended strategy for saving experiment configurations long term. You can see example config files in /robust_llm/hydra_conf/experiment. If you add a new file to /robust_llm/hydra_conf/experiment called `my_exp.yaml` then you can use it with:

```
python robust_llm +experiment=my_exp
```

The complete configuration used will be printed as `Configuration arguments:`. This description can be copied into a new file in the /robust_llm/hydra_conf/experiment directory if you want to repeat it later.

### Running batch jobs
If you have your hydra config prepared and want to run an experiment on the cluster in batch mode, you can use the `run_batch_job.py` script. To use it, make sure you check out the correct git commit that you want in your experiment, and that all the relevant changes are committed. The container will first set up a code directory with the commit matching your current repo, and then run the experiment. Usage:

```
python run_batch_job.py --hydra_config=<HYDRA_CONFIG_NAME> [--experiment_name=<EXP_NAME> --job_type=<JOB_TYPE> --container_tag=<TAG>]
```

As a requirement, you have to set up `docker` ([instructions](https://github.com/AlignmentResearch/flamingo/wiki/Docker-tutorial:-secure-credentials-and-basic-use#read-only-credentials-for-your-cluster-account)), `github-credentials` ([instructions](https://github.com/AlignmentResearch/flamingo/wiki/Build-Docker-images-on-the-cluster:-Kaniko#authentication-1-pulling-from-your-private-github-repo)), and `wandb` kubernetes secrets. For `wandb`, use the following command:

```
kubectl create secret generic wandb --from-literal=api-key=<YOUR_WANDB_API_KEY>
```

For now, we only support a single job. In the future we will probably add support for multiple jobs (like grid searches).

## Datasets

### Tomita
Tomita datasets must be pregenerated as files in order to be used in training.

They can be generated by running `robust_llm/dataset_management/tomita/tomita_dataset_generator.py`. You can edit the file's `__main__` function call to extend the range of examples to generate for training.

## Building Docker images
Docker images of the repo can be built using Kaniko. The first time you do so, you must follow the setup described [here](https://github.com/AlignmentResearch/flamingo/wiki/Build-Docker-images-on-the-cluster:-Kaniko), particularly setting up the Kubernetes secrets named `docker` and `github-credentials`.

To build a Docker image from the `main` branch at head, you can run:

```
kubectl create -f k8s/kaniko-build.yaml
```

If you wish to build a Docker image from a different branch, you should edit the `BRANCH_NAME` value in `k8s/kaniko-build.yaml` and then run the command above. More details can be found in the Flamingo wiki article on Kaniko, and the Kaniko docs themselves.
