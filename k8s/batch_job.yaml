# Based on https://github.com/AlignmentResearch/learned-planners/blob/main/k8s/runner.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: {NAME}
  labels:
    kueue.x-k8s.io/queue-name: farai
    wandb-project: {WANDB_PROJECT}
    wandb-entity: {WANDB_ENTITY}
    launch-id: {LAUNCH_ID}
spec:
  suspend: true
  template:
    metadata:
      generateName: {NAME}
    spec:
      priorityClassName: {PRIORITY}
      volumes:
        - name: robust-llm-storage
          persistentVolumeClaim:
            claimName: az-robust-llm
        - name: devshm
          emptyDir:
            medium: Memory
            sizeLimit: 4Gi
      containers:
        - name: batch-job
          image: "ghcr.io/alignmentresearch/robust-llm:{CONTAINER_TAG}"
          command:
            - bash
            - -c
            # nvidia-smi checks that GPU is available since we've had GPU
            # outages before that make the job silently run on CPU.
            # exec makes sure that parallel becomes PID 1 in the container so
            # that it receives signals from Kubernetes. --term-seq then tells
            # parallel to send SIGTERM to the processes when parallel is
            # interrupted and then wait 30s before sending SIGKILL.
            # We use a temporary file to read the commands from because piping
            # them from stdin didn't work well with exec.
            # We use _PARALLELDELIMITER_ as the delimiter for parallel to avoid
            # issues with newlines in the command and \0 being lost when writing to file.
            # --halt soon,fail=1 is there because 'parallel' was not exiting even when
            # both processes it ran had failed.
            - |
              set -euo pipefail
              nvidia-smi && \
              git clone https://github.com/AlignmentResearch/robust-llm.git && \
              cd robust-llm && \
              git checkout {COMMIT_HASH} && \
              git submodule update --recursive && \
              tmpfile=$(mktemp) && \
              echo -ne {COMMAND} > "$tmpfile" && \
              exec parallel -u --halt soon,fail=1 --delimiter _PARALLELDELIMITER_ --term-seq TERM,30000,KILL,25 --arg-file "$tmpfile"

          resources:
            requests:
              cpu: {CPU}
            limits:
              memory: {MEMORY}
              nvidia.com/gpu: {GPU}
          volumeMounts:
            - name: robust-llm-storage
              mountPath: /robust_llm_data
            # This is needed as for multi-GPU training, accelerate needs to write
            # to /dev/shm and the default size is too small
            - name: devshm
              mountPath: /dev/shm
          env:
            - name: GIT_ASKPASS
              value: "true"
            - name: GITHUB_PAT
              valueFrom:
                secretKeyRef:
                  name: github-credentials
                  key: pat
            - name: GIT_CONFIG_PARAMETERS
              value: "'credential.https://github.com.username=$(GITHUB_PAT)'"
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: wandb
                  key: api-key
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: openai-api-key
                  key: key
                  optional: true  # Key only needed for StrongREJECT ScoringFn.
            - name: WANDB_ENTITY
              value: {WANDB_ENTITY}
            - name: WANDB_PROJECT
              value: {WANDB_PROJECT}
            - name: WANDB_MODE
              value: {WANDB_MODE}
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface
                  key: token
      # Never restart the pod. If the job retries, it will do so in a new pod.
      restartPolicy: Never
      imagePullSecrets:
        - name: docker
  # If the job fails, retry up to 3 times with a new pod.
  # Job failures can happen due to transient errors such as outage of external services
  # we pull/push data to (HuggingFace, W&B, ...).
  backoffLimit: 3
  # Don't count pod disruption such as preemption towards backoff target.
  # These are dependent on cluster weather, and not our code. See
  # https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy
  podFailurePolicy:
    rules:
    - action: Ignore
      onPodConditions:
      - type: DisruptionTarget
